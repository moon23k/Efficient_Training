## Memory_Ablation

Recently, large-scale models are playing a prominent role in various AI tasks. However, as large models have high expressive power, they are expensive to compute. This causes problems both in train process and inference process.

The key to mend this problem is managing memory in efficient fashion. 
This repo presents a set of experiments for efficient GPU memory management.


<br>
<br>

## Train Strategies

**Gradient Accumulate**

<br>

**Gradient Checkpointing**

<br>

**Mixed precision training**

<br>

**Pruning**

<br>

**Tensor RT**

<br>

**Optimizer**

<br>
<br>

## Result

<br>
<br>

## Reference
**[LightSeq: A High Performance Inference Library for Transformers](https://arxiv.org/pdf/2010.13887.pdf)**
