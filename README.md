## Fast_Ablation

Recently, Transformer-Based Pretrained Language Models show great performances in various NLP tasks.

PLM shows excellent performance based on a large number of parameters.

Therefore, those consume excessive computational cost and time on training and inference processes.

To mend this problem, this repo covers how to keep the model lightweight while still maintaining acceptable performance. 

<br>
<br>

## Strategies

**Hugging Face**

<br>

**ByteDance**

<br>

**DeepSpeed**

<br>

**Turbo Transformers**

<br>

**Custom**

<br>
<br>

## Result

<br>
<br>

## Reference
**[LightSeq: A High Performance Inference Library for Transformers](https://arxiv.org/pdf/2010.13887.pdf)**
